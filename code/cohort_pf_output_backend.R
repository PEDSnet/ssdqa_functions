
########################## DATA PRE PROCESSING #####################################

#' Compute median fact counts by visit & fact type and by site, visit, & fact type
#'
#' @param data_input the list output of `combine_study_facts`
#' @param site_col the column in the data that holds site names
#' @param agegrp boolean that determines whether the output should also
#'               be grouped by age group
#' @param codeset boolean that determines whether the output should also
#'               be grouped by a user provided codeset flag
#'
#' @return dataframe that contains the total median number of facts for that visit & fact_type
#'         as well as the median number of facts for the site, visit, & fact type for each
#'         study 
#' 
compute_pf_medians <- function(data_input,
                               site_col,
                               agegrp=NULL,
                               codeset=NULL) {
  
  
  data_input_cols <- data_input %>% colnames()
  
  if('cohort' %in% data_input_cols) {
    data_input_grp <- 
      data_input %>% group_by(cohort)
  } else {data_input_grp <- data_input}
  
  if(is.data.frame(agegrp)) {data_input_grp <- data_input_grp %>% group_by(age_grp,.add=TRUE) %>% 
    mutate(age_grp = ifelse(is.na(age_grp), 'None', age_grp))}
  if(is.data.frame(codeset)) {data_input_grp <- data_input_grp %>% group_by(flag,.add=TRUE) %>%
    mutate(flag = ifelse(is.na(flag), 'None', flag))}
  
  site_distance_medians_tbl <- 
    data_input_grp %>% 
    group_by(study,
             visit_type,
             domain,
             .add=TRUE) %>% 
    mutate(median_all_with0s=median(var_val),
           median_all_without0s=median(var_val[var_val!=0])) %>% 
    ungroup()
  
  site_distance_final <- 
    data_input_grp %>% 
    left_join(site_distance_medians_tbl) %>% 
    group_by(study,
             !!sym(site_col),
             visit_type,
             domain,
             median_all_with0s,
             median_all_without0s,
             .add=TRUE) %>% 
    summarise(n_tot=n(),
              n_w_fact=sum(var_ever),
              median_site_with0s=median(var_val),
              median_site_without0s=median(var_val[var_val!=0])) %>% ungroup() #%>% 
    #mutate(prop_all_w_fact=round(n_w_fact/n_tot,3))
  
  site_distance_final <- 
    site_distance_final %>% replace(is.na(.), 0) %>% 
    mutate(across(everything(), ~ replace(.x, is.nan(.x), 0)))
  
  
  site_distance_final
  
}


#' Compute distance from mean
#'
#' @param data_input input table generated by `loop_through_visits` and reduced with `combine_study_facts`
#' @param site_col the column in the data that holds site names
#' @param n_sd the number of standard deviations that should be used as a threshold to
#'             identify an outlier
#' @param agegrp a csv file with user designated age groupings, based on age at cohort entry
#' @param codeset a csv file with user designated cohort flags based on a user provided codeset
#'
#' @return a dataframe that summarises the number of patients with fact counts that fall +/- 3 SD away from the mean
#'         both at fact and site + fact levels
#'         
#'         contains columns: person_id, start_date, end_date, fu, site, domain, var_val, var_ever, study,
#'                           visit_type, n_fact, zscore_fact, outlier_fact, prop_outlier_fact, n_site_fact,
#'                           zscore_site_fact, outlier_site_fact, prop_outlier_site_fact
#' 
compute_dist_mean_pf <- function(data_input,
                                 site_col,
                                 n_sd = 2,
                                 agegrp = NULL,
                                 codeset = NULL) {
  
  if(is.data.frame(agegrp)) {data_input <- data_input %>% group_by(age_grp,.add=TRUE) %>% 
    mutate(age_grp = ifelse(is.na(age_grp), 'None', age_grp))}
  if(is.data.frame(codeset)) {data_input <- data_input %>% group_by(flag,.add=TRUE) %>%
    mutate(flag = ifelse(is.na(flag), 'None', flag))}
  
  site_dist_means_tbl <- 
    data_input %>% 
    group_by(study,
             visit_type,
             domain,
             .add=TRUE) %>% 
    mutate(n_fact=n(),
           mean_fact=mean(var_val),
           sd_fact=sd(var_val),
           zscore_fact = ((var_val - mean_fact) / sd_fact),
           abs_z = abs(zscore_fact),
           outlier = case_when(abs_z > n_sd ~ 1L,
                               TRUE ~ 0L),
           outlier_fact = sum(outlier),
           prop_outlier_fact = round(outlier_fact / n_fact, 3)) %>% 
    ungroup() %>%
    select(-c(outlier, abs_z, mean_fact, sd_fact))
  
  
  site_dist_means_final <- 
    data_input %>% 
    left_join(site_dist_means_tbl) %>% 
    group_by(study,
             !!sym(site_col),
             visit_type,
             domain,
             n_fact,
             outlier_fact,
             prop_outlier_fact,
             .add=TRUE) %>% 
    summarise(n_site_fact=n(),
              mean_site_fact=mean(var_val),
              sd_site_fact=sd(var_val),
              zscore_site_fact = ((var_val - mean_site_fact) / sd_site_fact),
              abs_z = abs(zscore_site_fact),
              outlier = case_when(abs_z > n_sd ~ 1L,
                                  TRUE ~ 0L),
              outlier_site_fact = sum(outlier),
              prop_outlier_site_fact = round(outlier_site_fact / n_site_fact, 3)) %>% 
    ungroup() %>%
    select(-c(outlier, abs_z, mean_site_fact, sd_site_fact, zscore_site_fact)) %>% distinct()
  
  site_dist_means_final <- 
    site_dist_means_final %>% replace(is.na(.), 0) 
  
  site_dist_means_final
}


############################ Not Currently in Use ######################################

#' Prepare data for k-means computation
#'
#' @param dat input table generated by `compute_pf_medians`
#' @param output the numerical column in the data to be used in the computation
#' @param facet_vars the variables by which the data should be facetted
#'
#' @return a list of matricies (one per distinct grouping of facet variables) with 
#'         one median value per site and per domain
#' 
prep_kmeans <- function(dat,
                        output,
                        facet_vars) {
  
  kmeans_list <- list()
  
if(!is.null(facet_vars)){
  select_cols <- c('site', 'domain', output, facet_vars)
  
  #facet_list <- dat %>% select(!!sym(facet_var)) %>% distinct() %>% pull()
  
  kmeans_prep <- 
    dat %>% 
    select(!!!syms(select_cols)) %>%
    #filter(!!sym(facet_var) == facet_list[[i]]) %>% 
    pivot_wider(id_cols = site,
                names_from = c(domain, !!!syms(facet_vars)),
                values_from = !!sym(output)) 
  
  kmeans_prep <- 
    kmeans_prep %>% replace(is.na(.), 0) %>% 
    mutate(across(everything(), ~ replace(.x, is.nan(.x),0)))
  
  cols <- kmeans_prep %>% select(-site) %>% colnames()
  
  domains <- dat %>% select(domain) %>% distinct() %>% pull()
  
  prep_regex <- function(domains){
    domains = c(domains)
    domain_options = paste(domains, collapse = '|^')
    paste0("(^", domain_options, ")+_")
  }
  
  regex <- prep_regex(domains)
  
  facet_list <- lapply(cols, function(x){gsub(x = x, pattern = regex, replacement = '')}) %>% unique()
  
  #facet_list <- lapply(cols, function(x){sub("^[^_]*_", "", x)}) %>% unique()
  
  for(i in 1:length(facet_list)){
  
  kmeans_mat <- 
    kmeans_prep %>% 
    column_to_rownames(., var='site')
  
  facet_kmeans <- kmeans_mat[, grepl(facet_list[[i]], names(kmeans_mat))]
  
  kmeans_scaled <- scale(facet_kmeans) 
  
  kmeans_scaled[, !colSums(!is.finite(kmeans_scaled))]
  
  kmeans_final <- kmeans_scaled
  
  kmeans_list[[i]] <- list(kmeans_scaled,
                           facet_list[[i]])
  }
  
  }else{
    
    select_cols <- c('site', 'domain', output)
      
      kmeans_prep <- 
        dat %>% 
        select(all_of(select_cols)) %>%
        pivot_wider(id_cols = site,
                    names_from = domain,
                    values_from = !!sym(output)) 
      
      kmeans_prep <- 
        kmeans_prep %>% replace(is.na(.), 0) %>% 
        mutate(across(everything(), ~ replace(.x, is.nan(.x),0)))
      
      kmeans_mat <- 
        kmeans_prep %>% 
        column_to_rownames(., var='site')
      
      kmeans_scaled <- scale(kmeans_mat) 
      
      kmeans_scaled[, !colSums(!is.finite(kmeans_scaled))]
      
      kmeans_list[[1]] <- list(kmeans_scaled,
                               "K-Means Cluster Analysis")
    
  }
  
  kmeans_list
  
  }


#' K-Means Output Generation
#'
#' @param kmeans_list a list of output tables from `prep_kmeans`; one table per distinct 
#'                    grouping of facet variables
#' @param centers an integer that denotes the number of clusters that should be
#'                generated
#'
#' @return a list of graphs, with one graph per distinct grouping of facet variables, 
#'         where site and domain are the dimensions
#' 
produce_kmeans_output <- function(kmeans_list,
                                  centers){
  
  output_list <- list()

  for(i in 1:length(kmeans_list)){
    
    if(ncol(kmeans_list[[i]][[1]]) == 0 || is.null(kmeans_list[[i]][[1]])){next}
    
    set.seed(123)
    k <- kmeans(kmeans_list[[i]][[1]], centers=centers)
    output_list[[i]] <- fviz_cluster(k, data=kmeans_list[[i]][[1]],
                 main = paste0(paste0('K-Means Cluster Analysis: ', kmeans_list[[i]][[2]])),
                 ggtheme = theme_minimal()) + scale_color_ssdqa()
    }
  output_list
}


#' FOT Heuristic Check
#'
#' @param target_col the statistic that should be used to generate the output;
#'                   options based on user configurations are provided in the `parameter_summary`
#'                   csv file output by `pf_process`
#' @param tblx table output by `pf_process`, filtered to one domain
#' @param site_col defaults to `site`
#' @param time_col defaults to `start_date`
#' @param facet_var list of variables by which the user would like to facet the output;
#'                  should match the facets used in `check_fot_multisite`
#'
#' @return list of two tables:
#'           -@fot_heuristic: table with relevant descriptive columns (site + facet_vars) and
#'                            the heuristic check
#'           -@fot_heuristic_summary: table with a summary of the heuristic, including the mean, median,
#'                                    quantiles, stdev, and descriptive columns (site + facet_vars)
#' 
fot_check <- function(target_col,
                      tblx,
                      site_col='site',
                      time_col='start_date',
                      facet_var) {
  if(is.null(facet_var)){
    cols_to_keep <- c(eval(site_col),eval(time_col),'check')
    groupedlist1 <- c(time_col)
    groupedlist2 <- c(site_col)
  }else{
    cols_to_keep <- c(eval(site_col),eval(time_col),eval(facet_var),'check')
    groupedlist1 <- c(time_col, facet_var)
    groupedlist2 <- c(site_col, facet_var)
  }
  
  rv <- FALSE
  rv_agg <- FALSE
  #base tbl to make a network wide version of the check
  agg_check <- tblx %>% group_by(!!!syms(groupedlist1)) %>%
    summarise(!!sym(target_col) := sum(!!sym(target_col))) %>%
    ungroup() %>%
    mutate({{site_col}}:='all')  # I need a cheat sheet of when {{x}}/eval(x)/!!sym(x) will actually work
  
  #for (target_check in tblx %>% select(!!sym(check_col)) %>% distinct() %>% pull()) {
  for (target_site in tblx %>% select(!!sym(site_col)) %>% distinct() %>% pull()) {
    foo <- fot_check_calc(tblx %>%
                            filter(site==target_site),
                          site_col='site',
                          time_col,
                          target_col) %>% collect()
    if(!is.logical(rv)){
      rv <- union(rv, foo)
    } else {
      rv <- foo
    }
  }
  
  bar <- fot_check_calc(agg_check,
                        site_col,time_col,target_col) %>%
    select(cols_to_keep) %>% collect()
  if(!is.logical(rv_agg)){
    rv_agg <- union(rv_agg, bar)
  } else {
    rv_agg <- bar
  }
  
  rv_summary <- rv %>% group_by(!!!syms(groupedlist2)) %>%
    summarise(std_dev = sd(check,na.rm=TRUE),
              pct_25 = quantile(check,.25),
              pct_75 = quantile(check,.75),
              med = median(check),
              m = mean(check)) %>% ungroup() %>% collect()
  
  rv_summary_allsites <- rv_agg %>%
    filter(site=='all') %>% group_by(!!!syms(groupedlist2)) %>%
    summarise(std_dev = sd(check,na.rm=TRUE),
              pct_25 = quantile(check,.25),
              pct_75 = quantile(check,.75),
              med = median(check),
              m = mean(check)) %>% ungroup() %>% collect() %>%
    mutate(site='all')
  
  
  return(list(fot_heuristic= dplyr::union(rv %>% select(cols_to_keep),
                                          rv_agg),
              fot_heuristic_summary=dplyr::union(rv_summary,
                                                 rv_summary_allsites)))
}


#' FOT Heuristic Check Calculation
#'
#' @param tblx table output by `pf_process`, filtered to one domain
#' @param site_col defaults to `site`
#' @param time_col defaults to `start_date`
#' @param target_col the statistic that should be used to generate the output;
#'                   options based on user configurations are provided in the `parameter_summary`
#'                   csv file output by `pf_process`
#'
#' @return dataframe with normalized distribution for specified domain based on the following heuristic:
#'         
#'         (month)/((month-1) x .25+(month+1) x .25+(month-12) x .5)
#' 
fot_check_calc <- function(tblx, 
                           site_col = 'site',
                           time_col = 'start_date', 
                           target_col) {
  tblx %>%
    arrange(!! sym(site_col), !! sym(time_col)) %>% 
    mutate(
      lag_1 = lag(!!sym(target_col)),
      lag_1_plus = lead(!!sym(target_col),1),
      lag_12 = lag(!!sym(target_col),12),
      check_denom = (lag(!!sym(target_col))*.25 +
                       lead(!!sym(target_col),1)*.25 +
                       lag(!!sym(target_col),12)*.5)) %>%
    # check_denom()) %>% 
    filter(check_denom!=0) %>%
    mutate(check = !!sym(target_col)/check_denom-1)
}


#' fot table computing distance from "all" check
#'
#' @param fot_check_output first element of list output from `fot_check`
#'
#' @return tbl with the following columns:
#' domain | check_name | month_end | centroid | site | check | distance
#'
#' The `distance` column measures, for each site/domain/check/month combination,
#' the distance between the site's normalized `check` output compared to
#' all sites combined.
#'

check_fot_all_dist <- function(fot_check_output) {
  
  just_all <-
    fot_check_output %>%
    filter(site=='all') %>%
    rename(centroid=check) %>%
    select(-c(site))
  
  combined <-
    just_all %>%
    inner_join(
      fot_check_output
    ) %>% mutate(
      distance=round(check,3)-round(centroid,3)
    )
}

#' Execute all FOT functions
#' 
#' @param tblx dataframe output by `pf_process`
#' @param target_col the statistic that should be used to generate the output;
#'                   options based on user configurations are provided in the `parameter_summary`
#'                   csv file output by `pf_process`
#' @param site_col defaults to `site`
#' @param time_col defaults to `start_date`
#' @param domain_list list of available domains in `tblx` to be used in loop
#' @param facet_var list of variables by which the user would like to facet the output
#' 
#' @return dataframe with normalized distribution of facts for each site, domain, and facet variable
#'         based on the following heuristic
#'         
#'         (month)/((month-1) x .25+(month+1) x .25+(month-12) x .5)

check_fot_multisite <- function(tblx,
                                target_col,
                                site_col = 'site',
                                time_col = 'start_date', 
                                domain_list,
                                facet_var = NULL) {
  
  final_all <- list()
  
  for(i in 1:length(domain_list)) {
    
    tblx_input <- tblx %>% filter(domain == domain_list[[i]]) %>% 
      group_split(!!!syms(facet_var))
    
    temp_rslt <- list()
    
    for(k in 1:length(tblx_input)){
    fot_output <- fot_check(tblx=tblx_input[[k]] %>% ungroup(),
                            target_col=target_col,
                            site_col=site_col,
                            time_col=time_col,
                            facet_var=facet_var)
    
    fot_distance <- check_fot_all_dist(fot_output$fot_heuristic) %>% 
      mutate(grp_check=domain_list[[i]])
    
    temp_rslt[[k]] <- fot_distance
    
    }
    
    final_all[[i]] <- fot_distance
    
  }
  
  letsreduce <- reduce(.x=final_all,
                       .f=dplyr::union)
  
}

#' output of `check_fot_multisite` to look for points where
#' a site is +/- 2 MAD from the median/centoid
#' 
#' @param multisite_tbl a tbl with all sites and a `grp_check` column, as well as a `month_end`, 
#'                      `distance`, `site` columns; output from the `check_fot_multisite` function
#' @param facet_var list of variables by which the user would like to facet the output;
#'                  should match the facets used in `check_fot_multisite`
#' @param mad_dev an integer to define the deviation that should be used to compute the upper and lower MAD limits
#' 
#' @return dataframe that includes statistics relating to the number of outliers / anomalous measures are present
#'         in the data based on deviation from the MAD

produce_multisite_mad <- function(multisite_tbl,
                                  facet_var = NULL,
                                  mad_dev) {
  if(is.null(facet_var)){
    grp1 <- c('start_date', 'grp_check', 'centroid')
    grp2 <- c('site', 'grp_check')
  }else{
    grp1 <- c('start_date', 'grp_check', 'centroid', facet_var)
    grp2 <- c('site', 'grp_check', facet_var)
  }
  
  mad_computation <- 
    multisite_tbl %>% 
    group_by(!!!syms(grp1)) %>% 
    summarise(mad_pt=mad(check, center=centroid)) %>% 
    ungroup() %>% 
    mutate(lower_mad = mad_pt - (abs(mad_pt*mad_dev)),
           upper_mad = mad_pt + (abs(mad_pt*mad_dev)))
  
  full_tbl_outliers <- 
    multisite_tbl %>% ungroup() %>% 
    inner_join(mad_computation) %>% 
    mutate(
      outlier=case_when((distance < lower_mad) | (distance > upper_mad) ~ 1,
                        TRUE ~ 0)
    ) %>% filter(! site=='all')
  
  sites_grp_outliers <- 
    full_tbl_outliers %>% 
    group_by(!!!syms(grp2)) %>% 
    filter(outlier==1) %>% 
    summarise(grp_outlier_num=n()) %>%  ungroup() 
  
  sites_grp_ct_total <- 
    full_tbl_outliers %>% 
    group_by(!!!syms(grp2)) %>% 
    summarise(grp_total_num=n()) %>% ungroup()
  
  sites_grp_total <- 
    sites_grp_ct_total %>% 
    left_join(sites_grp_outliers) %>% 
    mutate(grp_outlier_prop = round(grp_outlier_num/grp_total_num,2))
  
  sites_total <- 
    sites_grp_total %>% ungroup() %>% 
    group_by(site) %>% 
    mutate(site_total_num=sum(grp_total_num, na.rm = TRUE),
           site_total_outlier=sum(grp_outlier_num, na.rm = TRUE)) %>% 
    mutate(site_outlier_prop=round(site_total_outlier/site_total_num,2)) %>% 
    rename(grp=grp_check)
  
}
